{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 4\n",
    "### Brendon Happ\n",
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from smart_open import smart_open\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_list = []\n",
    "\n",
    "for (dirname, dirs, files) in os.walk('clean_pickles'):\n",
    "    for filename in files:\n",
    "        with open(os.path.join('clean_pickles', filename), 'rb') as f:\n",
    "            full_df_list.append(pd.read_pickle(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat(full_df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'content', 'region', 'language', 'publish_date', 'following',\n",
       "       'followers', 'updates', 'post_type', 'account_type', 'retweet',\n",
       "       'account_category', 'new_june_2018', 'tweet_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_stopwords(df):\n",
    "#    df = ' '.join(word for word in df.split() if word not in words)\n",
    "#    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_col = full_df['content'].apply(remove_stopwords)\n",
    "#full_df.loc[:, 'content'] = clean_col.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = full_df.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, tokenize the documents, remove common words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2116866,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    '''\n",
    "    Tokenises, and lemmatize's using spacy. Returns a string of space seperated tokens.\n",
    "    '''\n",
    "    #words = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())\n",
    "    words = nlp(text)\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    result = []\n",
    "    global cache\n",
    "    for word in words:\n",
    "        # Memoization \n",
    "        if word in stops:\n",
    "            continue\n",
    "        elif len(word) == 1:\n",
    "            continue\n",
    "        elif len(word) == 2:\n",
    "            continue\n",
    "        elif word not in cache:\n",
    "            lemma = str(word.lemma_) if word.lemma_ != \"-PRON-\" else str(word)\n",
    "            cache[word] = lemma\n",
    "        else:\n",
    "            lemma = cache[word]\n",
    "        result.append(lemma)\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = []\n",
    "cache = {}\n",
    "for tweet in tweets:\n",
    "    tokenized_tweets.append(preprocess_text(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def ngrams_split(lst, n):\n",
    "#    return [' '.join(lst[i:i+n]) for i in range(len(lst)-n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram_tweets = []\n",
    "#for tweet in tokenized_tweets:\n",
    "#    bigram_tweets.append(ngrams_split(tweet.split(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic/Concept Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert documents to vectors, use a **bag of words** representation. In this representation, each document is represented by one vector where a vector element i represents the number of times the ith word appears in the document.\n",
    "\n",
    "It is advantageous to represent the questions only by their (integer) ids. The mapping between the questions and ids is called a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_tweets = []\n",
    "for tweet in tokenized_tweets:\n",
    "    tweet_list = tweet.split()\n",
    "    gensim_tweets.append(tweet_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add bigrams to gensim formatted tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrases = Phrases(gensim_tweets, min_count=3, threshold=100)\n",
    "#bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi_tweets = []\n",
    "#for sent in bigram[gensim_tweets]:\n",
    "#    bi_tweets.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bi_tweets.pickle', 'rb') as f:\n",
    "    bi_tweets = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Stops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tweets = []\n",
    "for sent in bi_tweets:\n",
    "    filtered_tweets.append([word for word in sent if word not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('filtered_bi_tweets.pickle', 'wb') as file:\n",
    "    pickle.dump(filtered_tweets, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2116866"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(396124 unique tokens: ['attack', 'bank', 'home', 'israeli', 'news']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(filtered_tweets)\n",
    "dictionary.save(os.path.join('temp_folder', 'tweets.dict'))  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter out words that appear in less than 100 documents and more than 50% of the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=100, no_above=0.5)\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The function doc2bow() simply counts the number of occurrences of each distinct word, converts the word to its integer word id and returns the result as a bag-of-words--a sparse vector, in the form of [(word_id, word_count), ...].**\n",
    "\n",
    "**doc2bow() has similar behaviors as calling transform() on CountVectorizer. doc2bow() can behave like fit_transform() as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_generator(lst, dictionary):\n",
    "    for i in lst: \n",
    "        yield dictionary.doc2bow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store to disk, for later use\n",
    "corpora.MmCorpus.serialize(os.path.join('temp_folder', 'tweets.mm'), bag_of_words_generator(filtered_tweets, dictionary))  \n",
    "corpus = corpora.MmCorpus(os.path.join('temp_folder', 'tweets.mm'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.num_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2116866"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaMulticore(corpus, num_topics=5, id2word=dictionary, workers=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.017*\"hillary\" + 0.016*\"video\" + 0.010*\"love\" + 0.010*\"say\" + 0.009*\"see\" + 0.009*\"make\" + 0.009*\"one\" + 0.008*\"medium\" + 0.008*\"world\" + 0.008*\"know\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.026*\"get\" + 0.016*\"people\" + 0.015*\"black\" + 0.013*\"like\" + 0.010*\"good\" + 0.008*\"life\" + 0.007*\"need\" + 0.007*\"think\" + 0.007*\"work\" + 0.007*\"white\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.031*\"world\" + 0.026*\"sport\" + 0.012*\"news\" + 0.012*\"new\" + 0.009*\"day\" + 0.008*\"win\" + 0.007*\"play\" + 0.007*\"game\" + 0.006*\"year\" + 0.006*\"first\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.046*\"news\" + 0.018*\"police\" + 0.016*\"man\" + 0.015*\"kill\" + 0.014*\"topnew\" + 0.013*\"world\" + 0.009*\"say\" + 0.008*\"shoot\" + 0.007*\"woman\" + 0.006*\"charge\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.043*\"trump\" + 0.015*\"obama\" + 0.011*\"topnew\" + 0.010*\"news\" + 0.010*\"president\" + 0.009*\"say\" + 0.008*\"world\" + 0.008*\"vote\" + 0.007*\"amp\" + 0.007*\"politic\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
